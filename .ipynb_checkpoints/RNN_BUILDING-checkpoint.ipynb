{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE have to predict the google stock price 'OPEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('/home/tejas/Desktop/DLAZ/Recurrent_Neural_Networks/Google_Stock_Price_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/10/2012</td>\n",
       "      <td>313.70</td>\n",
       "      <td>315.72</td>\n",
       "      <td>307.30</td>\n",
       "      <td>621.43</td>\n",
       "      <td>8,824,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1/11/2012</td>\n",
       "      <td>310.59</td>\n",
       "      <td>313.52</td>\n",
       "      <td>309.40</td>\n",
       "      <td>624.25</td>\n",
       "      <td>4,817,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1/12/2012</td>\n",
       "      <td>314.43</td>\n",
       "      <td>315.26</td>\n",
       "      <td>312.08</td>\n",
       "      <td>627.92</td>\n",
       "      <td>3,764,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1/13/2012</td>\n",
       "      <td>311.96</td>\n",
       "      <td>312.30</td>\n",
       "      <td>309.37</td>\n",
       "      <td>623.28</td>\n",
       "      <td>4,631,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1/17/2012</td>\n",
       "      <td>314.81</td>\n",
       "      <td>314.81</td>\n",
       "      <td>311.67</td>\n",
       "      <td>626.86</td>\n",
       "      <td>3,832,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1/18/2012</td>\n",
       "      <td>312.14</td>\n",
       "      <td>315.82</td>\n",
       "      <td>309.90</td>\n",
       "      <td>631.18</td>\n",
       "      <td>5,544,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1/19/2012</td>\n",
       "      <td>319.30</td>\n",
       "      <td>319.30</td>\n",
       "      <td>314.55</td>\n",
       "      <td>637.82</td>\n",
       "      <td>12,657,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1/20/2012</td>\n",
       "      <td>294.16</td>\n",
       "      <td>294.40</td>\n",
       "      <td>289.76</td>\n",
       "      <td>584.39</td>\n",
       "      <td>21,231,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1/23/2012</td>\n",
       "      <td>291.91</td>\n",
       "      <td>293.23</td>\n",
       "      <td>290.49</td>\n",
       "      <td>583.92</td>\n",
       "      <td>6,851,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1/24/2012</td>\n",
       "      <td>292.07</td>\n",
       "      <td>292.74</td>\n",
       "      <td>287.92</td>\n",
       "      <td>579.34</td>\n",
       "      <td>6,134,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1/25/2012</td>\n",
       "      <td>287.68</td>\n",
       "      <td>288.27</td>\n",
       "      <td>282.13</td>\n",
       "      <td>567.93</td>\n",
       "      <td>10,012,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1/26/2012</td>\n",
       "      <td>284.92</td>\n",
       "      <td>286.17</td>\n",
       "      <td>281.22</td>\n",
       "      <td>566.54</td>\n",
       "      <td>6,476,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1/27/2012</td>\n",
       "      <td>284.32</td>\n",
       "      <td>289.08</td>\n",
       "      <td>283.60</td>\n",
       "      <td>578.39</td>\n",
       "      <td>7,262,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1/30/2012</td>\n",
       "      <td>287.95</td>\n",
       "      <td>288.92</td>\n",
       "      <td>285.63</td>\n",
       "      <td>576.11</td>\n",
       "      <td>4,678,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1/31/2012</td>\n",
       "      <td>290.41</td>\n",
       "      <td>290.91</td>\n",
       "      <td>286.50</td>\n",
       "      <td>578.52</td>\n",
       "      <td>4,300,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2/1/2012</td>\n",
       "      <td>291.38</td>\n",
       "      <td>291.66</td>\n",
       "      <td>288.49</td>\n",
       "      <td>579.24</td>\n",
       "      <td>4,658,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2/2/2012</td>\n",
       "      <td>291.34</td>\n",
       "      <td>292.11</td>\n",
       "      <td>289.95</td>\n",
       "      <td>583.51</td>\n",
       "      <td>4,847,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2/3/2012</td>\n",
       "      <td>294.23</td>\n",
       "      <td>297.42</td>\n",
       "      <td>292.93</td>\n",
       "      <td>594.7</td>\n",
       "      <td>6,360,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2/6/2012</td>\n",
       "      <td>296.39</td>\n",
       "      <td>304.27</td>\n",
       "      <td>295.90</td>\n",
       "      <td>607.42</td>\n",
       "      <td>7,386,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2/7/2012</td>\n",
       "      <td>302.44</td>\n",
       "      <td>303.56</td>\n",
       "      <td>300.75</td>\n",
       "      <td>605.11</td>\n",
       "      <td>4,199,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2/8/2012</td>\n",
       "      <td>303.18</td>\n",
       "      <td>304.53</td>\n",
       "      <td>301.24</td>\n",
       "      <td>608.18</td>\n",
       "      <td>3,686,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2/9/2012</td>\n",
       "      <td>304.87</td>\n",
       "      <td>306.10</td>\n",
       "      <td>303.36</td>\n",
       "      <td>609.79</td>\n",
       "      <td>4,546,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2/10/2012</td>\n",
       "      <td>302.81</td>\n",
       "      <td>302.93</td>\n",
       "      <td>300.87</td>\n",
       "      <td>604.25</td>\n",
       "      <td>4,667,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2/13/2012</td>\n",
       "      <td>304.11</td>\n",
       "      <td>305.77</td>\n",
       "      <td>303.87</td>\n",
       "      <td>610.52</td>\n",
       "      <td>3,646,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2/14/2012</td>\n",
       "      <td>304.63</td>\n",
       "      <td>304.86</td>\n",
       "      <td>301.25</td>\n",
       "      <td>608.09</td>\n",
       "      <td>3,620,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>11/17/2016</td>\n",
       "      <td>766.92</td>\n",
       "      <td>772.70</td>\n",
       "      <td>764.23</td>\n",
       "      <td>771.23</td>\n",
       "      <td>1,304,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>11/18/2016</td>\n",
       "      <td>771.37</td>\n",
       "      <td>775.00</td>\n",
       "      <td>760.00</td>\n",
       "      <td>760.54</td>\n",
       "      <td>1,547,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>11/21/2016</td>\n",
       "      <td>762.61</td>\n",
       "      <td>769.70</td>\n",
       "      <td>760.60</td>\n",
       "      <td>769.2</td>\n",
       "      <td>1,330,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>11/22/2016</td>\n",
       "      <td>772.63</td>\n",
       "      <td>776.96</td>\n",
       "      <td>767.00</td>\n",
       "      <td>768.27</td>\n",
       "      <td>1,593,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>11/23/2016</td>\n",
       "      <td>767.73</td>\n",
       "      <td>768.28</td>\n",
       "      <td>755.25</td>\n",
       "      <td>760.99</td>\n",
       "      <td>1,478,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>11/25/2016</td>\n",
       "      <td>764.26</td>\n",
       "      <td>765.00</td>\n",
       "      <td>760.52</td>\n",
       "      <td>761.68</td>\n",
       "      <td>587,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>11/28/2016</td>\n",
       "      <td>760.00</td>\n",
       "      <td>779.53</td>\n",
       "      <td>759.80</td>\n",
       "      <td>768.24</td>\n",
       "      <td>2,188,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>11/29/2016</td>\n",
       "      <td>771.53</td>\n",
       "      <td>778.50</td>\n",
       "      <td>768.24</td>\n",
       "      <td>770.84</td>\n",
       "      <td>1,616,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>11/30/2016</td>\n",
       "      <td>770.07</td>\n",
       "      <td>772.99</td>\n",
       "      <td>754.83</td>\n",
       "      <td>758.04</td>\n",
       "      <td>2,392,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>12/1/2016</td>\n",
       "      <td>757.44</td>\n",
       "      <td>759.85</td>\n",
       "      <td>737.03</td>\n",
       "      <td>747.92</td>\n",
       "      <td>3,017,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>12/2/2016</td>\n",
       "      <td>744.59</td>\n",
       "      <td>754.00</td>\n",
       "      <td>743.10</td>\n",
       "      <td>750.5</td>\n",
       "      <td>1,452,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>12/5/2016</td>\n",
       "      <td>757.71</td>\n",
       "      <td>763.90</td>\n",
       "      <td>752.90</td>\n",
       "      <td>762.52</td>\n",
       "      <td>1,394,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>12/6/2016</td>\n",
       "      <td>764.73</td>\n",
       "      <td>768.83</td>\n",
       "      <td>757.34</td>\n",
       "      <td>759.11</td>\n",
       "      <td>1,690,700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>12/7/2016</td>\n",
       "      <td>761.00</td>\n",
       "      <td>771.36</td>\n",
       "      <td>755.80</td>\n",
       "      <td>771.19</td>\n",
       "      <td>1,761,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>12/8/2016</td>\n",
       "      <td>772.48</td>\n",
       "      <td>778.18</td>\n",
       "      <td>767.23</td>\n",
       "      <td>776.42</td>\n",
       "      <td>1,488,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>12/9/2016</td>\n",
       "      <td>780.00</td>\n",
       "      <td>789.43</td>\n",
       "      <td>779.02</td>\n",
       "      <td>789.29</td>\n",
       "      <td>1,821,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>12/12/2016</td>\n",
       "      <td>785.04</td>\n",
       "      <td>791.25</td>\n",
       "      <td>784.35</td>\n",
       "      <td>789.27</td>\n",
       "      <td>2,104,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>12/13/2016</td>\n",
       "      <td>793.90</td>\n",
       "      <td>804.38</td>\n",
       "      <td>793.34</td>\n",
       "      <td>796.1</td>\n",
       "      <td>2,145,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>12/14/2016</td>\n",
       "      <td>797.40</td>\n",
       "      <td>804.00</td>\n",
       "      <td>794.01</td>\n",
       "      <td>797.07</td>\n",
       "      <td>1,704,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>12/15/2016</td>\n",
       "      <td>797.34</td>\n",
       "      <td>803.00</td>\n",
       "      <td>792.92</td>\n",
       "      <td>797.85</td>\n",
       "      <td>1,626,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>12/16/2016</td>\n",
       "      <td>800.40</td>\n",
       "      <td>800.86</td>\n",
       "      <td>790.29</td>\n",
       "      <td>790.8</td>\n",
       "      <td>2,443,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>12/19/2016</td>\n",
       "      <td>790.22</td>\n",
       "      <td>797.66</td>\n",
       "      <td>786.27</td>\n",
       "      <td>794.2</td>\n",
       "      <td>1,232,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>12/20/2016</td>\n",
       "      <td>796.76</td>\n",
       "      <td>798.65</td>\n",
       "      <td>793.27</td>\n",
       "      <td>796.42</td>\n",
       "      <td>951,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>12/21/2016</td>\n",
       "      <td>795.84</td>\n",
       "      <td>796.68</td>\n",
       "      <td>787.10</td>\n",
       "      <td>794.56</td>\n",
       "      <td>1,211,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>12/22/2016</td>\n",
       "      <td>792.36</td>\n",
       "      <td>793.32</td>\n",
       "      <td>788.58</td>\n",
       "      <td>791.26</td>\n",
       "      <td>972,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>12/23/2016</td>\n",
       "      <td>790.90</td>\n",
       "      <td>792.74</td>\n",
       "      <td>787.28</td>\n",
       "      <td>789.91</td>\n",
       "      <td>623,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>12/27/2016</td>\n",
       "      <td>790.68</td>\n",
       "      <td>797.86</td>\n",
       "      <td>787.66</td>\n",
       "      <td>791.55</td>\n",
       "      <td>789,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>12/28/2016</td>\n",
       "      <td>793.70</td>\n",
       "      <td>794.23</td>\n",
       "      <td>783.20</td>\n",
       "      <td>785.05</td>\n",
       "      <td>1,153,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>12/29/2016</td>\n",
       "      <td>783.33</td>\n",
       "      <td>785.93</td>\n",
       "      <td>778.92</td>\n",
       "      <td>782.79</td>\n",
       "      <td>744,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>12/30/2016</td>\n",
       "      <td>782.75</td>\n",
       "      <td>782.78</td>\n",
       "      <td>770.41</td>\n",
       "      <td>771.82</td>\n",
       "      <td>1,770,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close      Volume\n",
       "0       1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1       1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2       1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3       1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4       1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
       "5      1/10/2012  313.70  315.72  307.30  621.43   8,824,000\n",
       "6      1/11/2012  310.59  313.52  309.40  624.25   4,817,800\n",
       "7      1/12/2012  314.43  315.26  312.08  627.92   3,764,400\n",
       "8      1/13/2012  311.96  312.30  309.37  623.28   4,631,800\n",
       "9      1/17/2012  314.81  314.81  311.67  626.86   3,832,800\n",
       "10     1/18/2012  312.14  315.82  309.90  631.18   5,544,000\n",
       "11     1/19/2012  319.30  319.30  314.55  637.82  12,657,800\n",
       "12     1/20/2012  294.16  294.40  289.76  584.39  21,231,800\n",
       "13     1/23/2012  291.91  293.23  290.49  583.92   6,851,300\n",
       "14     1/24/2012  292.07  292.74  287.92  579.34   6,134,400\n",
       "15     1/25/2012  287.68  288.27  282.13  567.93  10,012,700\n",
       "16     1/26/2012  284.92  286.17  281.22  566.54   6,476,500\n",
       "17     1/27/2012  284.32  289.08  283.60  578.39   7,262,000\n",
       "18     1/30/2012  287.95  288.92  285.63  576.11   4,678,400\n",
       "19     1/31/2012  290.41  290.91  286.50  578.52   4,300,700\n",
       "20      2/1/2012  291.38  291.66  288.49  579.24   4,658,700\n",
       "21      2/2/2012  291.34  292.11  289.95  583.51   4,847,400\n",
       "22      2/3/2012  294.23  297.42  292.93   594.7   6,360,700\n",
       "23      2/6/2012  296.39  304.27  295.90  607.42   7,386,700\n",
       "24      2/7/2012  302.44  303.56  300.75  605.11   4,199,700\n",
       "25      2/8/2012  303.18  304.53  301.24  608.18   3,686,400\n",
       "26      2/9/2012  304.87  306.10  303.36  609.79   4,546,300\n",
       "27     2/10/2012  302.81  302.93  300.87  604.25   4,667,700\n",
       "28     2/13/2012  304.11  305.77  303.87  610.52   3,646,100\n",
       "29     2/14/2012  304.63  304.86  301.25  608.09   3,620,900\n",
       "...          ...     ...     ...     ...     ...         ...\n",
       "1228  11/17/2016  766.92  772.70  764.23  771.23   1,304,000\n",
       "1229  11/18/2016  771.37  775.00  760.00  760.54   1,547,100\n",
       "1230  11/21/2016  762.61  769.70  760.60   769.2   1,330,600\n",
       "1231  11/22/2016  772.63  776.96  767.00  768.27   1,593,100\n",
       "1232  11/23/2016  767.73  768.28  755.25  760.99   1,478,400\n",
       "1233  11/25/2016  764.26  765.00  760.52  761.68     587,400\n",
       "1234  11/28/2016  760.00  779.53  759.80  768.24   2,188,200\n",
       "1235  11/29/2016  771.53  778.50  768.24  770.84   1,616,600\n",
       "1236  11/30/2016  770.07  772.99  754.83  758.04   2,392,900\n",
       "1237   12/1/2016  757.44  759.85  737.03  747.92   3,017,900\n",
       "1238   12/2/2016  744.59  754.00  743.10   750.5   1,452,500\n",
       "1239   12/5/2016  757.71  763.90  752.90  762.52   1,394,200\n",
       "1240   12/6/2016  764.73  768.83  757.34  759.11   1,690,700\n",
       "1241   12/7/2016  761.00  771.36  755.80  771.19   1,761,000\n",
       "1242   12/8/2016  772.48  778.18  767.23  776.42   1,488,100\n",
       "1243   12/9/2016  780.00  789.43  779.02  789.29   1,821,900\n",
       "1244  12/12/2016  785.04  791.25  784.35  789.27   2,104,100\n",
       "1245  12/13/2016  793.90  804.38  793.34   796.1   2,145,200\n",
       "1246  12/14/2016  797.40  804.00  794.01  797.07   1,704,200\n",
       "1247  12/15/2016  797.34  803.00  792.92  797.85   1,626,500\n",
       "1248  12/16/2016  800.40  800.86  790.29   790.8   2,443,800\n",
       "1249  12/19/2016  790.22  797.66  786.27   794.2   1,232,100\n",
       "1250  12/20/2016  796.76  798.65  793.27  796.42     951,000\n",
       "1251  12/21/2016  795.84  796.68  787.10  794.56   1,211,300\n",
       "1252  12/22/2016  792.36  793.32  788.58  791.26     972,200\n",
       "1253  12/23/2016  790.90  792.74  787.28  789.91     623,400\n",
       "1254  12/27/2016  790.68  797.86  787.66  791.55     789,100\n",
       "1255  12/28/2016  793.70  794.23  783.20  785.05   1,153,800\n",
       "1256  12/29/2016  783.33  785.93  778.92  782.79     744,300\n",
       "1257  12/30/2016  782.75  782.78  770.41  771.82   1,770,000\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date      1258\n",
       "Open      1258\n",
       "High      1258\n",
       "Low       1258\n",
       "Close     1258\n",
       "Volume    1258\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be the training set on which our RNN will be trained\n",
    "#Its will be containing the i/p data of our NN\n",
    "train_set = dataset_train['Open'].values\n",
    "#We can also do train_set=dataset_train.iloc[:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([325.25, 331.27, 329.83, ..., 793.7 , 783.33, 782.75])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "# 2 best ways are standardizatn. and normalizatn.\n",
    "\n",
    "#Whenever you build an RNN and theres a sigmoid functn. as the\n",
    "#activatn. functn. we will use normalizatn.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply the normalizatn. to the training set\n",
    "# .fit_transform will not only fit your obj. as sc to the training \n",
    "#set but will also scale it\n",
    "#train_set_scaled = sc.fit_transform(train_set)\n",
    "\n",
    "train_set_scaled = sc.fit_transform(train_set.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368],\n",
       "       [0.09701243],\n",
       "       [0.09433366],\n",
       "       ...,\n",
       "       [0.95725128],\n",
       "       [0.93796041],\n",
       "       [0.93688146]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are now going to create a data struct. that will specify what \n",
    "#the RNN will need th remember when predicting the next stock price.\n",
    "#And this is the no. of time steps.\n",
    "#Super imp. to have the right no. of time steps as wrong nos. could\n",
    "#lead to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data structure with 60 time steps and 1 o/p\n",
    "#Which means at each time t the RNN is going to look at 60 stock \n",
    "#prices and based on these 60 obs. it will try to predict the next \n",
    "#o/p i.e. the stock price at time t+1\n",
    "\n",
    "#First thing we need to do is create 2 seperate entities.\n",
    "#First is going to be X_train which will be the i/p of our NN\n",
    "#So basically for each financial day/obs. X_train will contain 60 \n",
    "#previews/stock prices before that financial day\n",
    "#Second will be y_train  will contain o/p i.e. stock price the next\n",
    "#financial day\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60,1258):\n",
    "    #as we have only 1 column in train_set_scaled we use 0 to \n",
    "    #specify it.\n",
    "    X_train.append(train_set_scaled[i-60:i, 0])\n",
    "    y_train.append(train_set_scaled[i, 0])\n",
    "    \n",
    "#Now we need to convert them into numpy arrays for our RNN\n",
    "X_train,y_train = np.array(X_train),np.array(y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198, 60)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "#Its 2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the data\n",
    "#i.e. adding more dimensionality\n",
    "\n",
    "#Any time you want to add dim. in a numpy array you always need to \n",
    "#use the reshape function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go to keras docs.->recurrent layers ->input shapes para\n",
    "#3D tensor with shape (batch_size, timesteps, input_dim)\n",
    "\n",
    "#batch_size - total no. of obs. we have(total stock prices from 2012 to 2016)\n",
    "#timesteps\n",
    "#input_dim - predictors\n",
    "X_train = np.reshape(X_train,newshape=(1198,60,1))\n",
    "#X_train = np.reshape(X_train,newshape=(X_train.shape[0],X_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198, 60, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the first LSTM layer and some dropout regularizatn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we need to input 3 imp. args.\n",
    "#units - no. of LSTM cells(neurons) you want in ur LSTM layer\n",
    "#return_sequences - set this to true bcoz we are building a stacked \n",
    "#LSTM which there4 will have several layers.As long as you keep adding\n",
    "#a layer you have to make it true\n",
    "#input_shape - shape of input(X_train).But we only have to input the\n",
    "#last 2 args. i.e. timesteps and indicators bcoz 1st one will be \n",
    "#automatically taken into account.\n",
    "\n",
    "#First LSTM layer\n",
    "regressor.add(LSTM(units=50,return_sequences=True,input_shape=(60,1)))\n",
    "regressor.add(Dropout(rate=0.2))\n",
    "\n",
    "#Second LSTM layer\n",
    "regressor.add(LSTM(units=50,return_sequences=True))\n",
    "regressor.add(Dropout(rate=0.2))\n",
    "\n",
    "#Third LSTM layer\n",
    "regressor.add(LSTM(units=50,return_sequences=True))\n",
    "regressor.add(Dropout(rate=0.2))\n",
    "\n",
    "#Fourth LSTM layer\n",
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the o/p layer \n",
    "\n",
    "#This is not an LSTM layer but a classic fully connected layer\n",
    "\n",
    "#As we are predicting only one value we have to put output dim.\n",
    "#(units) as 1\n",
    "regressor.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the RNN\n",
    "\n",
    "#for RNN RMSprop is preferred but adam can also be used.\n",
    "#for predicting continuous value loss will be mean square error\n",
    "regressor.compile(optimizer='adam',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 7s 6ms/step - loss: 0.0552\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0068\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0055\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0057\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0047\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0045\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0049\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0049\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0042\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0039\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 6s 5ms/step - loss: 0.0041\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 5s 5ms/step - loss: 0.0043\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 6s 5ms/step - loss: 0.0044\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0035\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 6s 5ms/step - loss: 0.0042\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0043\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0037\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0035\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0036\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0033\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0035\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0038\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 6s 5ms/step - loss: 0.0037\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0033\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0032\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0037\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0037\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0030\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0032\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0031\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0032\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0027\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0032\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0025\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0028\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0027\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0029\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0030\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0028\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0028\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0026\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0028\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0026\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0027\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0026\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0026\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0026\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0025\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0027\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0022\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0025\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0025\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0021\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0020\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0023\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0022\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0023\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0021\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0023\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0028\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0019\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0019\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0020\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0020\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0020\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0019\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0019\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0019\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0021\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0020\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0019\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0021\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0019\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0017\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0017\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0017\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0017\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0016\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0018\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0018\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0016\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0016\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0016\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0016\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0018\n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0018\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0016\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 4s 3ms/step - loss: 0.0015\n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0014\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0014\n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0017\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0016\n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0016\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 6s 5ms/step - loss: 0.0015\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0016\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0015\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 5s 4ms/step - loss: 0.0015\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 4s 4ms/step - loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f615480d748>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the RNN to training set\n",
    "regressor.fit(X_train,y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making preds. and visualizing results\n",
    "\n",
    "#Getting the test data \n",
    "dataset_test = pd.read_csv('/home/tejas/Desktop/DLAZ/Recurrent_Neural_Networks/Google_Stock_Price_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2017</td>\n",
       "      <td>778.81</td>\n",
       "      <td>789.63</td>\n",
       "      <td>775.80</td>\n",
       "      <td>786.14</td>\n",
       "      <td>1,657,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2017</td>\n",
       "      <td>788.36</td>\n",
       "      <td>791.34</td>\n",
       "      <td>783.16</td>\n",
       "      <td>786.90</td>\n",
       "      <td>1,073,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2017</td>\n",
       "      <td>786.08</td>\n",
       "      <td>794.48</td>\n",
       "      <td>785.02</td>\n",
       "      <td>794.02</td>\n",
       "      <td>1,335,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2017</td>\n",
       "      <td>795.26</td>\n",
       "      <td>807.90</td>\n",
       "      <td>792.20</td>\n",
       "      <td>806.15</td>\n",
       "      <td>1,640,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2017</td>\n",
       "      <td>806.40</td>\n",
       "      <td>809.97</td>\n",
       "      <td>802.83</td>\n",
       "      <td>806.65</td>\n",
       "      <td>1,272,400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date    Open    High     Low   Close     Volume\n",
       "0  1/3/2017  778.81  789.63  775.80  786.14  1,657,300\n",
       "1  1/4/2017  788.36  791.34  783.16  786.90  1,073,000\n",
       "2  1/5/2017  786.08  794.48  785.02  794.02  1,335,200\n",
       "3  1/6/2017  795.26  807.90  792.20  806.15  1,640,200\n",
       "4  1/9/2017  806.40  809.97  802.83  806.65  1,272,400"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = dataset_test['Open'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([778.81, 788.36, 786.08, 795.26, 806.4 , 807.86, 805.  , 807.14,\n",
       "       807.48, 807.08, 805.81, 805.12, 806.91, 807.25, 822.3 , 829.62,\n",
       "       837.81, 834.71, 814.66, 796.86])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predicted stock price for test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First thing to understand is that we trained our model to be able\n",
    "#to predict stock price at time t+1 based on the 60 previous stock \n",
    "#prices and therefore to predict each stock price of each financial\n",
    "#day of 2017(test set), we will need 60 previous stock prices of the\n",
    "#60 previous financial days b4 the actual day\n",
    "\n",
    "#Secondly, in order to get each day of jan 2017 the 60 previous \n",
    "#stock prices of 60 prev. days.\n",
    "#We will need both the train and test set bcoz only then we will \n",
    "#have some 60 days stock prices from the train set .\n",
    "\n",
    "#Thirdly, understand the way we have to make the concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat. original dfs\n",
    "#axis = 0 bcoz vertical concatenatn.\n",
    "dataset_total = pd.concat((dataset_train['Open'],dataset_test['Open']),axis=0)\n",
    "\n",
    "inputs = dataset_total[len(dataset_total)-len(dataset_test)-60:].values\n",
    "\n",
    "inputs = inputs.reshape(-1,1)\n",
    "\n",
    "inputs = sc.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "\n",
    "for i in range(60,80):\n",
    "    #as we have only 1 column in train_set_scaled we use 0 to \n",
    "    #specify it.\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "    \n",
    "#Now we need to convert them into numpy arrays for our RNN\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_test = np.reshape(X_test,newshape=(X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "pred = regressor.predict(X_test)\n",
    "\n",
    "#The predicted values will be scaled, so we have to inverse the \n",
    "#scaled values.\n",
    "pred = sc.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[781.4797 ],\n",
       "       [778.7155 ],\n",
       "       [778.7975 ],\n",
       "       [780.4242 ],\n",
       "       [783.96704],\n",
       "       [789.9549 ],\n",
       "       [795.6522 ],\n",
       "       [798.1791 ],\n",
       "       [798.44366],\n",
       "       [797.966  ],\n",
       "       [797.5837 ],\n",
       "       [797.31915],\n",
       "       [797.1347 ],\n",
       "       [797.55493],\n",
       "       [798.4143 ],\n",
       "       [802.7019 ],\n",
       "       [809.62665],\n",
       "       [817.27026],\n",
       "       [821.66064],\n",
       "       [817.8609 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizing the results\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6132eb9940>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_set,color = 'red',label='real')\n",
    "plt.plot(pred,color = 'green',label='pred')\n",
    "plt.title('Google stock price pred.')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google stock price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
